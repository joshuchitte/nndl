{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence and word tokenizer**"
      ],
      "metadata": {
        "id": "7Sh60JQlHXpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "text=\"In various contexts, the term 'world'takes a more restricted meaning associated, for example, with the Earth and all life on it, with humanity\"\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKsg2sgHuzD",
        "outputId": "a1984cc8-0117-4f08-c343-d372e10a15ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In various contexts, the term 'world'takes a more restricted meaning associated, for example, with the Earth and all life on it, with humanity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_MqnMH0Iiyx",
        "outputId": "f1f7eda3-b279-410a-ea04-148db6eecb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"In various contexts, the term 'world'takes a more restricted meaning associated, for example, with the Earth and all life on it, with humanity\"]\n",
            "['In', 'various', 'contexts', ',', 'the', 'term', \"'world'takes\", 'a', 'more', 'restricted', 'meaning', 'associated', ',', 'for', 'example', ',', 'with', 'the', 'Earth', 'and', 'all', 'life', 'on', 'it', ',', 'with', 'humanity']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tab Tokenizer**"
      ],
      "metadata": {
        "id": "xdC0YUF_I2Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TabTokenizer\n",
        "tk=TabTokenizer()\n",
        "text=\"Live for\\t doing good deeds..\\t.$$&* \\n not \\t for doing sins\"\n",
        "tab=tk.tokenize(text)\n",
        "print(tab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5_ioN6eIi2J",
        "outputId": "da307a82-249c-45e1-a8b3-0d8843f6c360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Live for', ' doing good deeds..', '.$$&* \\n not ', ' for doing sins']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Space Tokenizer**"
      ],
      "metadata": {
        "id": "lp0IVEV6JoA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import SpaceTokenizer\n",
        "tk=SpaceTokenizer()\n",
        "text=\"The subject contents related to NLP\"\n",
        "space=tk.tokenize(text)\n",
        "print(space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhJN9jtbIi40",
        "outputId": "558333a4-23d4-4670-92d1-da7f7e022e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'subject', 'contents', 'related', 'to', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line Tokenizer**"
      ],
      "metadata": {
        "id": "-tsqMJw2KFQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import LineTokenizer\n",
        "tk=LineTokenizer(blanklines ='keep')\n",
        "text=\"The price\\n\\n of burger \\nin Burger King is Rs.36.\\n\"\n",
        "line=tk.tokenize(text)\n",
        "print(text)\n",
        "print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efw9oOIzIi7w",
        "outputId": "b78ac209-c2c8-42f2-ffe0-665b3eb637b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price\n",
            "\n",
            " of burger \n",
            "in Burger King is Rs.36.\n",
            "\n",
            "['The price', '', ' of burger ', 'in Burger King is Rs.36.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuation Tokenizer**"
      ],
      "metadata": {
        "id": "BIihi5MxKsuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tk=WordPunctTokenizer()\n",
        "text=\"World Chocolate Day, Celebrated for the first time in 2009, commemorates the rich history of chocalte dating back to the Aztec period around '1400'BC. During\"\n",
        "punc=tk.tokenize(text)\n",
        "print(punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YOLc_1mIi_B",
        "outputId": "55e30e6e-7e22-4d2a-8027-9c9893e62720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['World', 'Chocolate', 'Day', ',', 'Celebrated', 'for', 'the', 'first', 'time', 'in', '2009', ',', 'commemorates', 'the', 'rich', 'history', 'of', 'chocalte', 'dating', 'back', 'to', 'the', 'Aztec', 'period', 'around', \"'\", '1400', \"'\", 'BC', '.', 'During']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Frequency Count**"
      ],
      "metadata": {
        "id": "Fjd6tc5LLXI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import ConditionalFreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "tk= ConditionalFreqDist()\n",
        "text=\"This world is very very very beautiful; because the people of this world are beautiful.\"\n",
        "for word in word_tokenize(text):\n",
        "  condition = len(word)\n",
        "  tk[condition][word] += 1\n",
        "tk[2]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhGb96q7LXSS",
        "outputId": "18011d31-13b9-4283-82d0-97ba67a58775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'is': 1, 'of': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}